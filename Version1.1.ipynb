{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1.1 RecModel Using Simple ANN's "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Libs needed for the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Activation, Dense, Input, Dropout\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import os, cv2\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "from keras.layers import Reshape, Conv2DTranspose, BatchNormalization\n",
    "from IPython.display import clear_output\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Clean Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the clean data after running the Cleaning_Data.py script to fill missing data and scalerize the features. This makes it easy to work with and make data discrete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>5</th>\n",
       "      <th>1</th>\n",
       "      <th>4</th>\n",
       "      <th>0.1</th>\n",
       "      <th>2</th>\n",
       "      <th>5.1</th>\n",
       "      <th>3</th>\n",
       "      <th>2.1</th>\n",
       "      <th>4.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  5  1  4  0.1  2  5.1  3  2.1  4.1\n",
       "0  0  3  1  4    0  0    0  3    2    4\n",
       "1  1  2  0  0    3  0    2  0    0    9\n",
       "2  1  3  1  0    4  0    2  0    0    2\n",
       "3  1  2  1  0    2  0    2  0    0    0\n",
       "4  1  4  1  0    4  0    2  0    0    2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Cleaned_Dataset.csv')\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate Data Features vs. Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seperate X_data represent the input values. 'PLACE' and 'target' represent the Credit card target. What should the model predict given X_data. Also this snipet of code converts Pandas => numpy/matrix data type to feed into model with the '.as_matrix()' function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223, 8)\n",
      "[[0.         0.50709255 0.16903085 ... 0.         0.         0.50709255]\n",
      " [0.23570226 0.47140452 0.         ... 0.         0.47140452 0.        ]\n",
      " [0.1796053  0.53881591 0.1796053  ... 0.         0.3592106  0.        ]\n",
      " ...\n",
      " [0.2        0.6        0.2        ... 0.         0.4        0.        ]\n",
      " [0.21320072 0.85280287 0.21320072 ... 0.         0.         0.        ]\n",
      " [0.21821789 0.65465367 0.21821789 ... 0.         0.         0.        ]]\n",
      "(223,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\Python3.6\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda\\envs\\Python3.6\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "FEATURES = dataset.columns[0:8]\n",
    "X_data = dataset[FEATURES].as_matrix()\n",
    "X_data = normalize(X_data)\n",
    "print(X_data.shape)\n",
    "print(X_data)\n",
    "PLACE = dataset.columns[9]\n",
    "target = dataset[PLACE].as_matrix()\n",
    "print(target.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snipet converts the single vector Y and turn it into a classification matrix with each class being the columns and the Rows being each Person/user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "num_classes = 14\n",
    "num_of_samples = X_data.shape[0]\n",
    "label = np.zeros((num_of_samples,num_classes),dtype='int64')\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the list to assign each proper label to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223, 14)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,223):\n",
    "    if target[i]== 0:\n",
    "        label[i,0]= 1\n",
    "    elif target[i] == 1:\n",
    "        label[i,1] = 1\n",
    "    elif target[i] == 2:\n",
    "        label[i,2] = 1\n",
    "    elif target[i] == 3:\n",
    "        label[i,3] = 1\n",
    "    elif target[i] == 4:\n",
    "        label[i,4] = 1\n",
    "    elif target[i] == 5:\n",
    "        label[i,5] = 1\n",
    "    elif target[i] == 6:\n",
    "        label[i,6] = 1\n",
    "    elif target[i] == 7:\n",
    "        label[i,7] = 1\n",
    "    elif target[i] == 8:\n",
    "        label[i,8] = 1\n",
    "    elif target[i] == 9:\n",
    "        label[i,9] = 1\n",
    "    elif target[i] == 10:\n",
    "        label[i,10] = 1\n",
    "    elif target[i] == 11:\n",
    "        label[i,11] = 1\n",
    "    elif target[i] == 12:\n",
    "        label[i,12] = 1\n",
    "    else:\n",
    "        label[i,13] = 1\n",
    "Y = label\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training set into each section for testing and training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y, test_size=0.3, random_state=1)\n",
    "print(y_test[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Fully connected Neural Network with LeakyRelu activations to prevent the neurons from 'dying' (precaution) Normalize after each layer training to be faster and more accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\Python3.6\\lib\\site-packages\\keras\\activations.py:115: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "D:\\Anaconda\\envs\\Python3.6\\lib\\site-packages\\keras\\activations.py:115: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "D:\\Anaconda\\envs\\Python3.6\\lib\\site-packages\\keras\\activations.py:115: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 14)                910       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 46,158\n",
      "Trainable params: 45,262\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\Python3.6\\lib\\site-packages\\keras\\activations.py:115: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation = LeakyReLU(alpha=0.1), input_shape=(8, )))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation = LeakyReLU(alpha=0.1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = LeakyReLU(alpha=0.1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(14, activation = LeakyReLU(alpha=0.1)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize The Training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        plt.savefig('figure.png', dpi = 300)\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently on the works => Preformance is bad and the learning rate is not optimized. However, training with SGD or ADAM should work. Just need to find proper Learning rate. No need for clipping norm since theres probably no exploding grads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_epochs = 100\n",
    "# Need help on fixing the training. The losses aren't changing as the learning rate goes up. Really weird. \n",
    "rmsprop = RMSprop(lr=1000, rho=0.9, epsilon=None, decay=0.01)\n",
    "model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit(X_train, y_train, batch_size=1, nb_epoch=training_epochs, verbose=1, validation_data=(X_test, y_test),  callbacks=[plot_losses],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First cell is used to execute and find the highest probability in the predicted result. This is indicated by the np.argmax(result) and then the GROUND TRUTH is found using the similar approach. The second snipet of code is just outputting the final classification of the credit card type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#The person we wish to check. \n",
    "test_subject = 1\n",
    "\n",
    "test_value = X_test [test_subject-1:test_subject, :]\n",
    "result = model.predict(test_value)\n",
    "print(result)\n",
    "num = np.argmax(result)\n",
    "real = np.argmax(y_test[test_subject-1:test_subject, :])\n",
    "print(y_test[test_subject-1:test_subject, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction\n",
      "Predicted: American Express Cobalt\n",
      "\n",
      "Ground Truth: \n",
      "Predicted: American Express Cobalt\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Prediction\")\n",
    "if num ==0:\n",
    "    print(\"Predicted: American Express Cobalt\")\n",
    "elif num ==1:\n",
    "    print(\"Predicted: American Express Essential\")\n",
    "elif num ==2:\n",
    "    print(\"Predicted: American Express Marriot Bonvoy\")\n",
    "elif num ==3:\n",
    "    print(\"Predicted: American Express Simply Cash\")\n",
    "elif num ==4:\n",
    "    print(\"Predicted: American Express Simply Cash Preferred\")\n",
    "elif num ==5:\n",
    "    print(\"Predicted: Meridian Visa Infinite\")\n",
    "elif num ==6:\n",
    "    print(\"Predicted: PC Financial Mastercard\")\n",
    "elif num ==7:\n",
    "    print(\"Predicted: Scotiabank Momentum No-Fee Visa\")\n",
    "elif num ==8:\n",
    "    print(\"Predicted: Scotiabank Momentum Visa Infinite\")\n",
    "elif num ==9:\n",
    "    print(\"Predicted: Scotiabank SCENE Visa\")\n",
    "elif num ==10:\n",
    "    print(\"Predicted: True Line Gold Mastercard\")\n",
    "elif num ==11:\n",
    "    print(\"Predicted: True Line Mastercard\")\n",
    "elif num ==12:\n",
    "    print(\"Predicted: American Express Blue Sky\")\n",
    "elif num ==13:\n",
    "    print(\"Predicted: Not Enough\")\n",
    "\n",
    "print()\n",
    "    \n",
    "print(\"Ground Truth: \")\n",
    "num = real\n",
    "if num ==0:\n",
    "    print(\"Predicted: American Express Cobalt\")\n",
    "elif num ==1:\n",
    "    print(\"Predicted: American Express Essential\")\n",
    "elif num ==2:\n",
    "    print(\"Predicted: American Express Marriot Bonvoy\")\n",
    "elif num ==3:\n",
    "    print(\"Predicted: American Express Simply Cash\")\n",
    "elif num ==4:\n",
    "    print(\"Predicted: American Express Simply Cash Preferred\")\n",
    "elif num ==5:\n",
    "    print(\"Predicted: Meridian Visa Infinite\")\n",
    "elif num ==6:\n",
    "    print(\"Predicted: PC Financial Mastercard\")\n",
    "elif num ==7:\n",
    "    print(\"Predicted: Scotiabank Momentum No-Fee Visa\")\n",
    "elif num ==8:\n",
    "    print(\"Predicted: Scotiabank Momentum Visa Infinite\")\n",
    "elif num ==9:\n",
    "    print(\"Predicted: Scotiabank SCENE Visa\")\n",
    "elif num ==10:\n",
    "    print(\"Predicted: True Line Gold Mastercard\")\n",
    "elif num ==11:\n",
    "    print(\"Predicted: True Line Mastercard\")\n",
    "elif num ==12:\n",
    "    print(\"Predicted: American Express Blue Sky\")\n",
    "elif num ==13:\n",
    "    print(\"Predicted: Not Enough\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
